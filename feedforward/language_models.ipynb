{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in creating summaries of texts. In order to do so, we want to find the probability distribution over the next word given all the words we have seen so far:\n",
    "\n",
    "$$ p(w_n | \\{w_i\\}_{i=1}^{n-1}) $$\n",
    "\n",
    "This is known as a language model: a probability distribution over sequences of words, or the next word in our case. We make a Markovian assumption that the probability of the next words depends only on the previous $m$ words seen:\n",
    "\n",
    "$$ p(w_n | \\{w_i\\}_{i=1}^{n-1})  \\approx p(w_n | \\{w_i\\}_{i=n - m+1}^{n-1}) $$\n",
    "\n",
    "For example, if we assume $m = 2$, i.e. that the distribution over the next word depends only on the two most recently seen words, we have\n",
    "\n",
    "$$ p(w_n | \\{w_i\\}_{i=1}^{n-1}) \\approx p(w_n | w_{n-1}, w_{n-2})$$\n",
    "\n",
    "For the rest of this note, we use $m=2$ but the code can easily be generalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical model we use is similar to the one we used for sentiment analysis:\n",
    "\n",
    "$$ p(w_n | w_{n-1}, w_{n-2}) \\propto \\exp\\{V f(w_{n-2}, w_{n-1}) + V_0\\}$$\n",
    "\n",
    "where $V \\in \\mathbb{R}^{|\\mathcal V| \\times h}$ is some weight matrix with hidden layer size $h$ and $V_0$ is our bias term. We define $f(w_{n-1}, w_{n-2})$ to be\n",
    "\n",
    "$$f(w_{n-2}, w_{n-1}) = \\text{tanh}(W[Ew_{n-2}, Ew_{n-1}]^\\top + W_0)$$\n",
    "\n",
    "where $E \\in \\mathbb{R}^{d \\times |\\mathcal V|}$ is our embedding matrix, $W \\in \\mathbb{R}^{(md) \\times h}$ another weight matrix, and $W_0$ another bias term. Then, we are interested in learning $V, W,$ and $E$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Vocab processing\n",
    "\n",
    "nV -- size of vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = require \"nn\"\n",
    "m = 2 -- size of \"memory\"\n",
    "d = 10 -- embedding size\n",
    "h  -- hidden layer size\n",
    "\n",
    "model = nn.Sequential()\n",
    "lookup = nn.LookupTable(d, nV)\n",
    "model:add(lookup) -- matrix E\n",
    "model:add(nn.Linear(m*d, h)) -- V and V_0\n",
    "model:add(Tanh()) -- activation function\n",
    "model:add(nn.Linear(nV, h)) -- W and W_0\n",
    "model:add(nn.LogSoftMax()) -- log softmax\n",
    "\n",
    "criterion = nn.CLassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Training via SGD w/ minibatch\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "step_size = .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i = 1, epochs do\n",
    "    nll = 0\n",
    "    for j = 1, train:size(1)/batch_size do\n",
    "        model:zeroGradParameters()\n",
    "        input = train:narrow\n",
    "        output = train_t:narrow\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- perplexity: exp{-1/N sum_i(ln(q(x_i)))}\n",
    "-- for each data point, generate distribution over words q\n",
    "-- find the probability of the actual word, q(x_i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
